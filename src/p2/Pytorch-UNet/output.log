nohup: ignoring input
INFO: Using device cuda
INFO: Network:
	3 input channels
	7 output channels (classes)
	Transposed conv upscaling
Using cache found in /home/ymtuan/.cache/torch/hub/milesial_Pytorch-UNet_master
/home/ymtuan/.pyenv/versions/3.8.20/lib/python3.8/site-packages/torch/cuda/__init__.py:654: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
INFO: Creating dataset with 2000 examples
INFO: Unique mask values: [0, 1, 2, 3, 4, 5, 6]
wandb: WARNING `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id 8roov5u0.
wandb: Tracking run with wandb version 0.21.4
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /home/ymtuan/dl/dlcv-fall-2025-hw1-ymtuan/src/p2/Pytorch-UNet/wandb/offline-run-20250918_200837-8roov5u0
INFO: Starting training:
        Epochs:          100
        Batch size:      32
        Learning rate:   0.001
        Training size:   1600
        Validation size: 400
        Checkpoints:     True
        Device:          cuda
        Images scaling:  1.0
        Mixed Precision: False
    
train.py:80: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)
Epoch 1/100:   0%|          | 0/1600 [00:00<?, ?img/s]Epoch 1/100:   0%|          | 0/1600 [00:02<?, ?img/s]
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /home/ymtuan/dl/dlcv-fall-2025-hw1-ymtuan/src/p2/Pytorch-UNet/wandb/offline-run-20250918_200837-8roov5u0[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250918_200837-8roov5u0/logs[0m
Traceback (most recent call last):
  File "train.py", line 259, in <module>
    train_model(
  File "train.py", line 121, in train_model
    grad_scaler.scale(loss).backward()
  File "/home/ymtuan/.pyenv/versions/3.8.20/lib/python3.8/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/ymtuan/.pyenv/versions/3.8.20/lib/python3.8/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/ymtuan/.pyenv/versions/3.8.20/lib/python3.8/site-packages/torch/autograd/graph.py", line 768, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: NVML_SUCCESS == DriverAPI::get()->nvmlInit_v2_() INTERNAL ASSERT FAILED at "../c10/cuda/CUDACachingAllocator.cpp":806, please report a bug to PyTorch. 
